Inference Optimization: For PyTorch Users
=============================================

* `How to accelerate a PyTorch inference pipeline through ONNXRuntime <accelerate_pytorch_inference_onnx.html>`_
* `How to accelerate a PyTorch inference pipeline through OpenVINO <accelerate_pytorch_inference_openvino.html>`_
* `How to accelerate a PyTorch inference pipeline through JIT/IPEX <accelerate_pytorch_inference_jit_ipex.html>`_
* `How to accelerate a PyTorch inference pipeline through multiple instances <multi_instance_pytorch_inference.html>`_
* `How to quantize your PyTorch model for inference using Intel Neural Compressor <quantize_pytorch_inference_inc.html>`_
* `How to quantize your PyTorch model for inference using OpenVINO Post-training Optimization Tools <quantize_pytorch_inference_pot.html>`_
* |pytorch_inference_context_manager_link|_
* `How to save and load optimized IPEX model <pytorch_save_and_load_ipex.html>`_
* `How to save and load optimized JIT model <pytorch_save_and_load_jit.html>`_
* `How to save and load optimized ONNXRuntime model <pytorch_save_and_load_onnx.html>`_
* `How to save and load optimized OpenVINO model <pytorch_save_and_load_openvino.html>`_
* `How to find accelerated method with minimal latency using InferenceOptimizer <inference_optimizer_optimize.html>`_

.. |pytorch_inference_context_manager_link| replace:: How to use context manager through ``get_context``
.. _pytorch_inference_context_manager_link: pytorch_context_manager.html